{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Download and prepare the dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Creating a model\n",
    "def create_cnn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compiling a model\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Creating and compiling the model\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "# Training the model\n",
    "history_cnn = cnn_model.fit(x_train, y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Model assessment\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy of CNN: {test_acc_cnn:.4f}')\n",
    "\n",
    "# Visualizing learning and validation curves\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curves')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy Curves')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_history(history_cnn)\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred_cnn = cnn_model.predict(x_test)\n",
    "y_pred_classes_cnn = np.argmax(y_pred_cnn, axis=1)\n",
    "y_true_cnn = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix_cnn = confusion_matrix(y_true_cnn, y_pred_classes_cnn)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix_cnn, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - CNN')\n",
    "plt.show()\n",
    "\n",
    "# Analyze misclassified images\n",
    "misclassified_indices_cnn = np.where(y_pred_classes_cnn != y_true_cnn)[0]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, incorrect in enumerate(misclassified_indices_cnn[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_test[incorrect].reshape(28, 28), cmap='gray', interpolation='none')\n",
    "    plt.title(f\"True: {y_true_cnn[incorrect]}, Predicted: {y_pred_classes_cnn[incorrect]}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"To compare the accuracy of the\n",
    "   convolutional neural network (CNN) \n",
    "   with the accuracy of the multilayer \n",
    "   network (MLP) from the previous \n",
    "   task, we can look at the test_acc\n",
    "   values of both models.\n",
    "\n",
    "   - Accuracy of a multilayer network (MLP):\n",
    "     Test accuracy: <значення точності MLP>\n",
    "\n",
    "   - Convolutional Neural Network (CNN) accuracy:\n",
    "     Test accuracy of CNN: <значення точності CNN>\n",
    "\n",
    "   If the accuracy of CNN is higher than that of MLP, it will indicate that\n",
    "   CNN performs better on the task of classifying images from the Fashion\n",
    "   MNIST dataset. \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  General conclusions.\n",
    "\n",
    "   - The Convolutional Neural Network (CNN) performed better in classifying images from\n",
    "     the Fashion MNIST dataset compared to the Multilayer Network (MLP)\n",
    "\n",
    "   - The CNN was more efficient in learning and identifying important image features,\n",
    "     resulting in higher classification accuracy.\n",
    "\n",
    "   - Regularization and the use of convolutional layers significantly improved the model's\n",
    "     ability to cope with data complexity.\n",
    "     \n",
    "   - The use of CNNs is more appropriate for image classification tasks due to its \n",
    "     architecture, which is able to automatically detect and utilize the spatial\n",
    "     features of the input data.\n",
    "     \n",
    "     Thus, for image classification tasks from the Fashion MNIST dataset, a convolutional\n",
    "   neural network is a more appropriate choice due to its ability to process and analyze \n",
    "   visual data in a high-quality manner.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import  GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Download and prepare the dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = np.stack((x_train,)*3, axis=-1)  # Convert black and white images to three-channel\n",
    "x_test = np.stack((x_test,)*3, axis=-1)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Prepare the data generator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(x_train)\n",
    "\n",
    "# Loading the basic model VGG16 without top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(28, 28, 3))\n",
    "\n",
    "# Freezing base layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Adding top layers for classification\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilation of the model\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training of upper layers\n",
    "history = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                    epochs=20,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "# Unfreeze some layers of the base model for additional training\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compilation of a model for pre-learning with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Relearning the model\n",
    "history_fine = model.fit(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                         epochs=10,\n",
    "                         validation_data=(x_test, y_test))\n",
    "\n",
    "# Model assessment\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f'Test accuracy of VGG16-based CNN: {test_acc:.4f}')\n",
    "\n",
    "# Visualizing learning and validation curves\n",
    "def plot_history(histories):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    for history in histories:\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss Curves')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy Curves')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "plot_history([history, history_fine])\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.arange(10), yticklabels=np.arange(10))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix - VGG16-based CNN')\n",
    "plt.show()\n",
    "\n",
    "# Analyze misclassified images\n",
    "misclassified_indices = np.where(y_pred_classes != y_true)[0]\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, incorrect in enumerate(misclassified_indices[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_test[incorrect].reshape(28, 28, 3), cmap='gray', interpolation='none')\n",
    "    plt.title(f\"True: {y_true[incorrect]}, Predicted: {y_pred_classes[incorrect]}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\"\"\"Conclusions\n",
    "   \n",
    "   - The use of the pre-trained VGG16 model as the basis for Fashion\n",
    "     MNIST image classification allowed us to achieve high accuracy.\n",
    "   \n",
    "   - Training of the upper layers and subsequent retraining of some \n",
    "     layers of the base model improved the classification accuracy.\n",
    "     \n",
    "   - Data augmentation helped to increase the diversity of the training\n",
    "     set, which contributed to the overall performance of the model.\n",
    "     \n",
    "   - Comparison with the previous models (MLP and CNN without VGG16) \n",
    "     showed that the use of pre-trained models significantly improves \n",
    "     classification results.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Conclusions from the second task\n",
    "\n",
    "   - Efficiency of the use of roll-up layers:\n",
    "      Convolutional neural networks significantly outperform multilayer \n",
    "     perceptrons in image classification tasks due to their ability to\n",
    "     automatically detect and process spatial features.\n",
    "     \n",
    "   - The advantages of pre-trained models:\n",
    "      The use of pre-trained models, such as VGG16, allows for higher\n",
    "     accuracy by transferring learned features from large datasets.\n",
    "      Training additional layers based on a specific dataset (Fashion MNIST)\n",
    "     helps to further improve accuracy.\n",
    "     \n",
    "   - Data augmentation:\n",
    "      The use of data augmentation techniques helps to increase the\n",
    "     diversity of the training set, which helps to avoid overfitting and\n",
    "     improve the overall performance of the model.\n",
    "     \n",
    "   - Data augmentation:\n",
    "      The use of data augmentation techniques helps to increase the\n",
    "     diversity of the training set, which helps to avoid overfitting and\n",
    "     improve the overall performance of the model.\n",
    "     \n",
    "   - Застосування та практика:\n",
    "      For image classification tasks, the use of CNNs, especially those based\n",
    "     on pre-trained models, is the most effective approach.\n",
    "      The practice of data retraining and augmentation is crucial for\n",
    "     achieving high accuracy in specific tasks.\n",
    "     \n",
    "     In general, the use of VGG16-based CNN for image clathe Fashion MNIST dataset has ssification from\n",
    "    achieved the highest accuracy among all the models under consideration, which confirms the\n",
    "    effectiveness of deep convolutional neural networks and transfer learning methods.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
